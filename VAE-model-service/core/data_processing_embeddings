import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder , MinMaxScaler, OneHotEncoder
import numpy as np
from gensim.models import Word2Vec
from collections.abc import Sequence
import torch
import torch.nn as nn
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import CountVectorizer



def load_data(file_path : str) -> pd.DataFrame:
    df = pd.read_csv(file_path)
    dff = pd.DataFrame(df)
    return df


critical_columns = [
        'Layer3_Network_SourceIP',
        'Layer3_Network_DestinationIP',
        'Layer3_Network_Protocol',
        'Layer4_Transport_SourcePort',
        'Layer4_Transport_DestinationPort'
    ]


# 1. Handle missing values (preserve all features)
# Fill numerical missing values with 0
def PreProcessingData(df : pd.DataFrame) -> pd.DataFrame:
  num_cols = df.select_dtypes(include=['int64', 'float64']).columns
  df[num_cols] = df[num_cols].fillna(0)
  # Remove rows with missing critical information
  cleaned_df = df.dropna(subset=critical_columns)
  # Optional: Log removed packets for analysis
  print(f"Original dataset size: {len(df)}")
  print(f"Cleaned dataset size: {len(cleaned_df)}")
  print(f"Packets removed: {len(df) - len(cleaned_df)}")
  return cleaned_df





# Positional Encoding class
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x



class NetworkDataProcessor:
    def __init__(self, vector_size=100):
        self.label_encoder = LabelEncoder()
        self.min_max_scaler = MinMaxScaler()
        self.word2vec_model = None
        self.vector_size = vector_size
        self.vectorizer = CountVectorizer(token_pattern=r'\b\w+\b')

    def _preprocess_text(self, text_series):
        """Preprocess text columns"""
        return text_series.fillna('UNKNOWN').astype(str)

    def _encode_categorical(self, series):
        """Encode categorical data"""
        return self.label_encoder.fit_transform(self._preprocess_text(series))

    def _scale_numeric(self, series):
        """Scale numeric data"""
        return self.min_max_scaler.fit_transform(series.values.reshape(-1, 1)).flatten()

    def _create_word2vec_model(self, corpus):
        """Create Word2Vec model"""
        tokenized_corpus = [str(text).split() for text in corpus]
        self.word2vec_model = Word2Vec(
            sentences=tokenized_corpus, 
            vector_size=self.vector_size, 
            window=5, 
            min_count=1, 
            workers=4
        )

    def _text_to_embedding(self, text_series):
        """Convert text to embeddings"""
        if self.word2vec_model is None:
            raise ValueError("Word2Vec model not initialized")
        
        embeddings = []
        for text in text_series:
            words = str(text).split()
            word_vecs = [self.word2vec_model.wv[word] for word in words if word in self.word2vec_model.wv]
            embedding = np.mean(word_vecs, axis=0) if word_vecs else np.zeros(self.vector_size)
            embeddings.append(embedding)
        
        return np.array(embeddings)

    def process_dataframe(self, df):
        """Process entire DataFrame"""
        processed_df = df.copy()
        
        # Text columns for embedding
        text_columns = [
            'Layer2_DataLink_SourceMAC', 'Layer2_DataLink_DestinationMAC', 
            'Layer3_Network_SourceIP', 'Layer3_Network_DestinationIP',
            'Layer3_Network_Protocol', 'Payload_ASCII', 'Payload_Hex'	
        ]
        
        # Create corpus for Word2Vec
        corpus = processed_df[text_columns].apply(lambda row: ' '.join(row.astype(str)), axis=1)
        self._create_word2vec_model(corpus)

        for col in processed_df.columns:
            if df[col].dtype == 'object':
                # Embedded text columns
                if col in text_columns:
                    processed_df[col] = self._text_to_embedding(df[col])
                # Other categorical columns
                else:
                    processed_df[col] = self._encode_categorical(df[col])
            
            # Numeric columns
            elif np.issubdtype(df[col].dtype, np.number):
                processed_df[col] = self._scale_numeric(df[col])

        return processed_df

# Usage
def process_network_data(file_path):
    df = pd.read_csv(file_path)
    df = PreProcessingData(df)
    processor = NetworkDataProcessor()
    processed_df = processor.process_dataframe(df)
    processed_df.to_csv('processed_network_data.csv', index=False)
    return processed_df

# Example
if __name__ == '__main__':
    processed_data = process_network_data('packet_data.csv')
    print(processed_data.head())






